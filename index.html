<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>OPPO US Research Center - XR Algorithms Team</title>

    <meta name="author" content="OPPO US Research Center, XR Team">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/3000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  OPPO US Research Center
                </p>
                <p class="name" style="text-align: center;">
                  XR Algorithms Team
                </p>
                <p>We are part of a multi-national and cross-functional OPPO XR team that developed multiple XR devices, including the OPPO AR Glass 2021 and the OPPO MR Glass Developer Edition, among others.
                </p>
                <p style="text-align:center">
                  <a href="https://www.oppo.com/en/">OPPO Official Site</a> &nbsp;/&nbsp;
                  <a href="https://github.com/oppo-us-research">Github</a> &nbsp;/&nbsp;
                  <a href="https://apply.workable.com/innopeaktech/">Career</a>
                </p>
              </td>
              <!-- <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/OPPO LOGO.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/OPPO LOGO.png" class="hoverZoomLink"></a>
              </td> -->
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Highlighted Research</h2>
                <p>
                We focus on 3D Computer Vision and Computer Graphics and their use cases for XR.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
      <tr onmouseout="SpacetimeGaussians_stop()" onmouseover="SpacetimeGaussians_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/stgauss.png' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://oppo-us-research.github.io/SpacetimeGaussians-website/">
            <span class="papertitle">Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis.</span>
          </a>
          <br>
          <a href="https://lizhan17.github.io/web/">Zhan Li</a>,
          <a href="https://zhangchen8.github.io/">Zhang Chen</a>,
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>,
          <br>
          <em>IEEE Conference on Computer Vision and Pattern Recognition<b> (CVPR)</b></em>, 2024
          <br>
          <p></p>

          <a href="https://oppo-us-research.github.io/SpacetimeGaussians-website/">project page</a>
          / 
          <a href="https://github.com/oppo-us-research/SpacetimeGaussians">code</a>
          /
          <a href="https://arxiv.org/abs/2312.16812">pdf</a>

        </td>
      </tr>

      <tr onmouseout="Naruto_stop()" onmouseover="Naruto_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/naurato.png' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://oppo-us-research.github.io/NARUTO-website/">
            <span class="papertitle">NARUTO:Neural Active Reconstruction from Uncertain Target Observations.</span>
          </a>
          <br>
          <a href="https://ziyue.cool/">Ziyue Feng</a>,
          <a href="https://huangying-zhan.github.io/"> Huangying Zhan</a>,
          <a href="https://scholar.google.com/citations?user=X6MkScIAAAAJ&hl=en"> Zheng Chen</a>,
          <a href="https://yanqingan.github.io/">Qingan Yan</a>,
          <a href="https://scholar.google.com/citations?hl=en&user=Pt4q_QkAAAAJ">Xiangyu Xu</a>,
          <a href="https://www.changjiangcai.com/"> Changjiang Cai</a>,
          <a href="https://www.clemson.edu/cecas/departments/automotive-engineering/people/li.html">Bing Li</a>,
          <a href="https://www.clemson.edu/cecas/departments/automotive-engineering/people/qzhu.html">Qilun Zhu</a>,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>.
          <br>
          <em>IEEE Conference on Computer Vision and Pattern Recognition<b> (CVPR)</b></em>, 2024
          <br>
          <p></p>

          <a href="https://oppo-us-research.github.io/NARUTO-website/">project page</a>
          / 
          <a href="https://github.com/oppo-us-research/NARUTO">code</a>
          /
          <a href="https://arxiv.org/pdf/2402.18771.pdf">pdf</a>
        </td>
      </tr>


      <tr onmouseout="Spect24_stop()" onmouseover="Spect24_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/cvpr24mesh.png' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://oppo-us-research.github.io/mesh-website/">
            <span class="papertitle">Spectrum AUC Difference (SAUCD): Human-aligned 3D Shape Evaluation.</span>
          </a>
          <br>
          <a href="https://tyluann.github.io/">Tianyu Luan</a>,
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
          <a href="">Lele Chen</a>,
          <a href="">Xuan Gong</a>,
          <a href="">Lichang Chen</a>,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>,
          <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>
          <br>
          <em>IEEE Conference on Computer Vision and Pattern Recognition<b> (CVPR)</b></em>, 2024
          <br>
          <p></p>

          <a href="https://arxiv.org/pdf/2405.06822.pdf">pdf</a>
        </td>
      </tr>


      <tr onmouseout="smyface_stop()" onmouseover="smyface_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/showmyface.png' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="">
            <span class="papertitle">Show Your Face: Restoring Complete Facial Images from Partial Observations for VR Meeting.</span>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?user=X6MkScIAAAAJ&hl=en"> Zheng Chen</a>,
          <a href="">Zhiqi Zhang</a>,
          <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>,
          <a href="">Lantao Liu</a>.
          <br>
          <em>IEEE/CVF Winter Conference<b> (WACV)</b></em>, 2024
          <br>
          <p></p>
          <a href="https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Show_Your_Face_Restoring_Complete_Facial_Images_From_Partial_Observations_WACV_2024_paper.pdf">pdf</a>
        </td>
      </tr>

      <tr onmouseout="NEC_stop()" onmouseover="NEC_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/nec.png' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="">
            <span class="papertitle">Stereo-NEC: Enhancing Stereo Visual-Inertial SLAM Initialization with Normal Epipolar Constraints.</span>
          </a>
          <br>
          <a href="">Weihan Wang</a>,
          <a href="">Chieh Chou</a>,
          <a href="">Ganesh Sevagamoorthy</a>,
          <a href="">Kevin Chenb</a>,
          <a href="">Zheng Chen</a>,
          <a href="">Ziyue Feng</a>,
          <a href="">Youjie Xia</a>,
          <a href=""> Feiyang Cai</a>,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>,
          <a href="">Philippos Mordohai</a>.
          <br>
          <em>IEEE/CVF Winter Conference<b> (WACV)</b></em>, 2024
          <br>
          <p></p>
          <a href="https://arxiv.org/pdf/2403.07225">pdf</a>
        </td>
      </tr>


      <tr onmouseout="NeuRBF_stop()" onmouseover="NeuRBF_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/2023_neurbf.gif' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://oppo-us-research.github.io/NeuRBF-website/">
            <span class="papertitle">NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions.</span>
          </a>
          <br>
          <a href="https://zhangchen8.github.io/">Zhang Chen</a>,
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
          <a href="https://lsongx.github.io/">Liangchen Song</a>,
          <a href="https://lelechen63.github.io/">Lele Chen</a>,
          <a href="http://www.yu-jingyi.com/cv/">Jingyi Yu</a>,
          <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>.
          <br>
          <em>IEEE International Conference on Computer Vision<b> (ICCV)</b></em>, 2023<b> (Oral Presentation)</b>
          <br>
          <p></p>

          <a href="https://oppo-us-research.github.io/NeuRBF-website/">project page</a>
          / 
          <a href="https://github.com/oppo-us-research/NeuRBF">code</a>
          / 
          <a href="https://arxiv.org/pdf/2312.16812.pdf">pdf</a>
        </td>
      </tr>

      <tr onmouseout="USST_stop()" onmouseover="USST_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/USST.png' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://actionlab-cv.github.io/EgoHandTrajPred/">
            <span class="papertitle">Uncertainty-aware State Space Transformer for Egocentric 3D Hand Trajectory Forecasting.</span>
          </a>
          <br>
          <a href="https://cogito2012.github.io/homepage/">Wentao Bao</a>,
          <a href="https://lelechen63.github.io/">Lele Chen</a>,
          <a href="https://libingzeng.github.io/">Libing Zeng</a>,
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>,
          <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>,
          <a href="https://www.egr.msu.edu/~yukong/">Yu Kong</a>.
          <br>
          <em>IEEE International Conference on Computer Vision<b> (ICCV)</b></em>, 2023
          <br>
          <p></p>

          <a href="https://actionlab-cv.github.io/EgoHandTrajPred/">project page</a>
          / 
          <a href="https://github.com/oppo-us-research/USST">code</a>
          / 
          <a href="https://arxiv.org/pdf/2307.08243.pdf">arXiv</a>
        </td>
      </tr>

      <tr onmouseout="Relit-NeuLF_stop()" onmouseover="Relit-NeuLF_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/Relit-NeuLF.gif' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://oppo-us-research.github.io/RelitNeuLF-website/">
            <span class="papertitle">Relit-NeuLF: Efficient Relighting and Novel View Synthesis via Neural 4D Light Field.</span>
          </a>
          <br>
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
          <a href="https://lsongx.github.io/">Liangchen Song</a>,
          <a href="https://zhangchen8.github.io/">Zhang Chen</a>, Xiangyu Du,
          <a href="https://lelechen63.github.io/">Lele Chen</a>,
          <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>
          <br>
          <em>ACM International Conf. on Multimedia<strong> (ACM MM)</strong> </em>, 2023
          <br>
          <p></p>

          <a href="https://oppo-us-research.github.io/RelitNeuLF-website/">project page</a>
          / 
          <a href="https://github.com/oppo-us-research/RelitNeuLF">code</a>
          / 
          <a href="https://arxiv.org/pdf/2310.14642.pdf">arXiv</a>
        </td>
      </tr>


      <tr onmouseout="NeRFPlayer_stop()" onmouseover="NeRFPlayer_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/flames-long.gif' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://lsongx.github.io/projects/nerfplayer.html">
            <span class="papertitle">NeRFPlayer: A Streamable Dynamic Scene Representation with Decomposed Neural Radiance Fields.</span>
          </a>
          <br>
          <a href="https://lsongx.github.io/">Liangchen Song</a>,
          <a href="https://apchenstu.github.io/">Anpei Chen</a>,
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
          <a href="https://zhangchen8.github.io/">Zhang Chen</a>,
          <a href="https://lelechen63.github.io/">Lele Chen</a>,
          <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>,
          <a href="https://www.cvlibs.net/">Andreas Geiger</a>.
          <br>
          <em>IEEE Conference on Virtual Reality and 3D User Interface<strong> (IEEE VR)</strong> </em>, 2023
          <em>Special Issue on IEEE Transactions on Visualization & Computer Graphics<strong> (TVCG)</strong> </em>, 2023
          <br>
          <p></p>

          <a href="https://lsongx.github.io/projects/nerfplayer.html">project page</a>
          / 
          <a href="https://arxiv.org/pdf/2210.15947.pdf">arXiv</a>
        </td>
      </tr>



      <tr onmouseout="OpenIllumination_stop()" onmouseover="OpenIllumination_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/OPENILLUMvideo_optimized.gif' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://oppo-us-research.github.io/OpenIllumination/">
            <span class="papertitle">OpenIllumination: A Multi-Illumination Dataset for Inverse Rendering Evaluation on Real Objects</span>
          </a>
          <br>
          <a href="https://www.liuisabella.com/">Isabella Liu</a>,
          <a href="https://ootts.github.io/">Linghao Chen</a>, Ziyang Fu, Liwen Wu,
          <a href="https://haian-jin.github.io/">Haian Jin</a>,
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>, Chin Ming Ryan Wong, 
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>,
          <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
          <a href="https://cseweb.ucsd.edu/~zex014">Zexiang Xu</a>,
          <a href="http://ai.ucsd.edu/~haosu/">Hao Su</a>.
          <br>
          <em>To appear at Neural Information Processing Systems<strong> (NeurIPS)</strong> </em>, 2023 &nbsp, Datasets and Benchmarks track
          <br>
          <p></p>

          <a href="https://oppo-us-research.github.io/OpenIllumination/">project page</a>
          /
          <a href="https://github.com/oppo-us-research/OpenIlluminationCapture">code</a>
          /
          <a href="https://arxiv.org/pdf/2309.07921.pdf">arXiv</a>
        </td>
      </tr>
      

      <tr onmouseout="CVPR2023_Landmark_stop()" onmouseover="CVPR2023_Landmark_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/cvpr23-3.gif' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://people.engr.tamu.edu/nimak/Papers/CVPR2023_Landmark/index.html">
            <span class="papertitle">3D-aware Facial Landmark Detection via Multiview Consistent Training on Synthetic Data. </span>
          </a>
          <br>
          <a href="https://libingzeng.github.io/">Libing Zeng</a>,
          <a href="https://lelechen63.github.io/">Lele Chen</a>,
          <a href="https://cogito2012.github.io/homepage/">Wentao Bao</a>,
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>,
          <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>,
          <a href="https://people.engr.tamu.edu/nimak/index.html">Nima Kalantari</a>.
          <br>
          <em>IEEE Conference on Computer Vision and Pattern Recognition<b> (CVPR)</b></em>, 2023
          <br>
          <p></p>

          <a href="https://people.engr.tamu.edu/nimak/Papers/CVPR2023_Landmark/index.html">project page</a>
          /
          <a href="">code(TBA)</a>
          /
          <a href="https://people.engr.tamu.edu/nimak/Papers/CVPR2023_Landmark/landmark/landmark_cvpr.pdf">arXiv</a>
        </td>
      </tr>

      <tr onmouseout="CVPR2023_hand_stop()" onmouseover="CVPR2023_hand_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/cvpr23_hand.png' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
            <span class="papertitle">High Fidelity 3D Hand Shape Reconstruction via Scalable Graph Frequency Decomposition </span>
          <br>
          <a href="https://tyluann.github.io/">Tianyu Luan</a>, Yuanhao Zhai, Jingjing Meng,
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
          <a href="https://zhangchen8.github.io/">Zhang Chen</a>,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>,
          <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>.
          <br>
          <em>IEEE Conference on Computer Vision and Pattern Recognition<b> (CVPR)</b></em>, 2023
          <br>
          <p></p>

          <a href="https://cse.buffalo.edu/~jsyuan/papers/2023/6034_high_fidelity_3d_hand_shape_re-Camera-ready%20PDF.pdf">arXiv</a>
        </td>
      </tr>
      
      <tr onmouseout="RIAV_MVS_stop()" onmouseover="RIAV_MVS_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/cvpr23_riav.jpg' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="http://www.changjiangcai.com/riav-mvs.github.io/">
            <span class="papertitle">RIAV-MVS: Recurrent-Indexing an Asymmetric Volume for Multi-View Stereo</span>
          </a>
          <br>
          
        <a href="https://www.changjiangcai.com/">Changjiang Cai</a>, 
        <a href="https://sites.google.com/view/panji530">Pan Ji</a>, 
        <a href="https://yanqingan.github.io/">Qingan Yan</a>, 
        <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>.  
          <br>
          <em>IEEE Conference on Computer Vision and Pattern Recognition<b> (CVPR)</b></em>, 2023
          <br>
          <p></p>

          <a href="http://www.changjiangcai.com/riav-mvs.github.io/">project page</a>
          /
          <a href="https://www.youtube.com/watch?v=7nvtlz0Caso&ab_channel=VisionCai">video</a>
          /
          <a href="https://arxiv.org/abs/2205.14320">arXiv</a>
        </td>
      </tr>

      <tr onmouseout="WACV2023_stop()" onmouseover="WACV2023_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/WACV_2023.png' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="papertitle">Semantics-Depth-Symbiosis: Deeply Coupled Semi-Supervised Learning of Semantics and Depth.</span>
          <br>
          <a href="https://scholar.google.com/citations?user=h_Hym_8AAAAJ&hl=en">Nitin Bansal</a>, 
          <a href="https://sites.google.com/view/panji530">Pan Ji</a>, 
          <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>. 
          <br>
          <em>IEEE Winter Conference on Computer Vision<b> (WACV)</b></em>, 2023
          <br>
          <p></p>

          <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Bansal_Semantics-Depth-Symbiosis_Deeply_Coupled_Semi-Supervised_Learning_of_Semantics_and_Depth_WACV_2023_paper.pdf">arXiv </a>
        </td>
      </tr>

      <tr onmouseout="georefine1_stop()" onmouseover="georefine1_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/eccv22_georefine1.jpg' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">

            <span class="papertitle">GeoRefine: Self-Supervised Online Depth Refinement for Accurate Dense Mapping</span>
          <br>
          <a href="https://sites.google.com/view/panji530">Pan Ji</a>, 
          <a href="https://yanqingan.github.io/">Qingan Yan</a>, Yuxin Ma,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu.</a>  
          <br>
          <em>European Conference on Computer Vision<b> (ECCV)</b></em>, 2022
          <br>
          <p></p>

          <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610354.pdf">arXiv</a>
          /
          <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610354-supp.pdf">Supplemental Material</a>
        </td>
      </tr>


      <tr onmouseout="PlaneMVS_stop()" onmouseover="PlaneMVS_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/cvpr22_plane.jpg' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
            <span class="papertitle">PlaneMVS: 3D Plane Reconstruction from Multi-View Stereo</span>
          <br>
        Jiachen Liu,
        <a href="https://sites.google.com/view/panji530">Pan Ji</a>, 
        <a href="https://scholar.google.com/citations?user=h_Hym_8AAAAJ&hl=en">Nitin Bansal</a>, 
        <a href="https://www.changjiangcai.com/">Changjiang Cai</a>, 
        <a href="https://yanqingan.github.io/">Qingan Yan</a>, Xiaolei Huang, 
        <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>.
          <br>
          <em>IEEE Conference on Computer Vision and Pattern Recognition<b> (CVPR)</b></em>, 2022
          <br>
          <p></p>

          <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_PlaneMVS_3D_Plane_Reconstruction_From_Multi-View_Stereo_CVPR_2022_paper.pdf">arXiv</a>
        </td>
      </tr>


      <tr onmouseout="NeuLF_stop()" onmouseover="NeuLF_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/xiaobu.gif' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://oppo-us-research.github.io/NeuLF-website/">
            <span class="papertitle">NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field. </span>
          </a>
          <br>
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
          <a href="https://lsongx.github.io/">Liangchen Song</a>,
          <a href="https://www.cct.lsu.edu/~cliu/">Celong Liu</a>,
          <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>.  
          <br>
          <em>Eurographics Symposium on Rendering<b> (EGSR)</b></em>, 2022
          <br>
          <p></p>

          <a href="https://oppo-us-research.github.io/NeuLF-website/">project page</a>
          /
          <a href="https://youtu.be/sD61yIfN6To">video</a>
          /
          <a href="https://arxiv.org/pdf/2105.07112.pdf">arXiv</a>
        </td>
      </tr>

      <tr onmouseout="TVCG2022_stop()" onmouseover="TVCG2022_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/tvcg_2022.png' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="papertitle">Real-Time Lighting Estimation for Augmented Reality via Differentiable Screen-Space Rendering.</span>
          <br>
          <a href="https://www.cct.lsu.edu/~cliu/">Celong Liu</a>, Lingyu Wang, 
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>, Shuxue Quan, 
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>.  
          <br>
          <em>IEEE Transactions on Visualization & Computer Graphics<b> (TVCG)</b></em>, 2022
          <br>
          <p></p>

          <a href="https://www.computer.org/csdl/journal/tg/5555/01/09678000/1A4SuYWCI7K">arXiv</a>
        </td>
      </tr>

      <tr onmouseout="POPNET_stop()" onmouseover="POPNET_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/popnet.png' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="papertitle">PoP-Net: Pose over Parts Network for Multi-Person 3D Pose Estimation from a Depth Image.</span>
          <br>
          <a href="https://scholar.google.com/citations?user=CP-YkUwAAAAJ&hl=en">Yuliang Guo</a>,
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>, Zekun Li, Xiangyu Du, Shuxue Quan,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>.  
          <br>
          <em>IEEE Winter Conference on Computer Vision<b> (WACV)</b></em>, 2022
          <br>
          <p></p>

          <a href="https://arxiv.org/pdf/2012.06734.pdf">arXiv </a>/
          <a href="https://github.com/oppo-us-research/PoP-Net">dataset</a>
        </td>
      </tr>

      <tr onmouseout="MonoIndoor_stop()" onmouseover="MonoIndoor_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/iccv21_Mono.png' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
            <span class="papertitle">MonoIndoor: Towards Good Practice of Self-Supervised Monocular Depth Estimation for Indoor Environments.</span>
          <br>
        <a href="https://sites.google.com/view/panji530">Pan Ji</a>, Runze Li, Bir Bhanu,
        <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>.
          <br>
          <em>IEEE International Conference on Computer Vision<b> (ICCV)</b></em>, 2021
          <br>
          <p></p>

          <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Ji_MonoIndoor_Towards_Good_Practice_of_Self-Supervised_Monocular_Depth_Estimation_for_ICCV_2021_paper.pdf">arXiv</a>
        </td>

      
    </table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:right;font-size:small;">
            Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron</a>
            <br>
            Last updated: June, 2024
          </p>
        </td>
      </tr>
    </table>
  </td>
</tr>
</table>
  </body>
</html>
