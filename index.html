<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>OPPO US Research Center, XR Team</title>

    <meta name="author" content="OPPO US Research Center, XR Team">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/3000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  OPPO US Research Center
                </p>
                <p class="name" style="text-align: center;">
                  XR Team
                </p>
                <p>We are a multi-national and cross-functional team that developed multiple XR devices. Since 2019, OPPO has developed OPPO AR Glass (AIO prototype), OPPO AR Glass 2021 (dev-kit, phone-tethered), OPPO Air Glass (a product in the China market, smart monocle), OPPO Air Glass 2 (prototype, binocular display, smart glass), and OPPO MR Glass Developer Edition.
                </p>
                <p style="text-align:center">
                  <a href="https://www.oppo.com/en/">OPPO Official Site</a> &nbsp;/&nbsp;
                  <a href="https://github.com/oppo-us-research">Github</a> &nbsp;/&nbsp;
                  <a href="https://apply.workable.com/innopeaktech/">Career</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/OPPO LOGO.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/OPPO LOGO.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  Our team focus on XR related projects, and are also interested in 3D Computer Vision and Graphics.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          

      <tr onmouseout="NeuRBF_stop()" onmouseover="NeuRBF_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/2023_neurbf.gif' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://oppo-us-research.github.io/NeuRBF-website/">
            <span class="papertitle">NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions.</span>
          </a>
          <br>
          <a href="https://zhangchen8.github.io/">Zhang Chen</a>,
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
          <a href="https://lsongx.github.io/">Liangchen Song</a>,
          <a href="https://lelechen63.github.io/">Lele Chen</a>,
          <a href="http://www.yu-jingyi.com/cv/">Jingyi Yu</a>,
          <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>.
          <br>
          <em>IEEE International Conference on Computer Vision<b>(ICCV)</b></em>, 2023<b> (Oral Presentation)</b>
          <br>
          <p></p>

          <a href="https://oppo-us-research.github.io/NeuRBF-website/">project page</a>
          / 
          <a href="https://github.com/oppo-us-research/NeuRBF">code</a>
          / 
          <a href="">arXiv</a>
        </td>
      </tr>

      <tr onmouseout="NeRFPlayer_stop()" onmouseover="NeRFPlayer_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/flames-long.gif' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://lsongx.github.io/projects/nerfplayer.html">
            <span class="papertitle">NeRFPlayer: A Streamable Dynamic Scene Representation with Decomposed Neural Radiance Fields.</span>
          </a>
          <br>
          <a href="https://lsongx.github.io/">Liangchen Song</a>,
          <a href="https://apchenstu.github.io/">Anpei Chen</a>,
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
          <a href="https://zhangchen8.github.io/">Zhang Chen</a>,
          <a href="https://lelechen63.github.io/">Lele Chen</a>,
          <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>,
          <a href="https://www.cvlibs.net/">Andreas Geiger</a>.
          <br>
          <em>IEEE Conference on Virtual Reality and 3D User Interface<strong>(IEEE VR)</strong> </em>, 2023
          <em>Special Issue on IEEE Transactions on Visualization & Computer Graphics<strong>(TVCG)</strong> </em>, 2023
          <br>
          <p></p>

          <a href="https://lsongx.github.io/projects/nerfplayer.html">project page</a>
          / 
          <a href="https://arxiv.org/pdf/2210.15947.pdf">arXiv</a>
        </td>
      </tr>



      <tr onmouseout="OpenIllumination_stop()" onmouseover="OpenIllumination_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/OPENILLUMvideo_optimized.gif' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://oppo-us-research.github.io/OpenIllumination/">
            <span class="papertitle">OpenIllumination: A Multi-Illumination Dataset for Inverse Rendering Evaluation on Real Objects</span>
          </a>
          <br>
          <a href="https://www.liuisabella.com/">Isabella Liu</a>,
          <a href="https://ootts.github.io/">Linghao Chen</a>, Ziyang Fu, Liwen Wu,
          <a href="https://haian-jin.github.io/">Haian Jin</a>,
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>, Chin Ming Ryan Wong, 
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>,
          <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
          <a href="https://cseweb.ucsd.edu/~zex014">Zexiang Xu</a>,
          <a href="http://ai.ucsd.edu/~haosu/">Hao Su</a>.
          <br>
          <em>To appear at Neural Information Processing Systems <strong>(NeurIPS)</strong> </em>, 2023 &nbsp, Datasets and Benchmarks track
          <br>
          <p></p>

          <a href="https://oppo-us-research.github.io/OpenIllumination/">project page</a>
          /
          <a href="https://github.com/oppo-us-research/OpenIlluminationCapture">code</a>
          /
          <a href="https://arxiv.org/pdf/2309.07921.pdf">arXiv</a>
        </td>
      </tr>
      

      <tr onmouseout="CVPR2023_Landmark_stop()" onmouseover="CVPR2023_Landmark_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/cvpr23-3.gif' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://people.engr.tamu.edu/nimak/Papers/CVPR2023_Landmark/index.html">
            <span class="papertitle">3D-aware Facial Landmark Detection via Multiview Consistent Training on Synthetic Data. </span>
          </a>
          <br>
          <a href="https://libingzeng.github.io/">Libing Zeng</a>,
          <a href="https://lelechen63.github.io/">Lele Chen</a>,
          <a href="https://cogito2012.github.io/homepage/">Wentao Bao</a>,
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>,
          <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>,
          <a href="https://people.engr.tamu.edu/nimak/index.html">Nima Kalantari</a>.
          <br>
          <em>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b></em>, 2023
          <br>
          <p></p>

          <a href="https://people.engr.tamu.edu/nimak/Papers/CVPR2023_Landmark/index.html">project page</a>
          /
          <a href="">code(TBA)</a>
          /
          <a href="https://people.engr.tamu.edu/nimak/Papers/CVPR2023_Landmark/landmark/landmark_cvpr.pdf">arXiv</a>
        </td>
      </tr>

      <tr onmouseout="CVPR2023_hand_stop()" onmouseover="CVPR2023_hand_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/cvpr23_hand.png' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
            <span class="papertitle">High Fidelity 3D Hand Shape Reconstruction via Scalable Graph Frequency Decomposition </span>
          <br>
          <a href="https://tyluann.github.io/">Tianyu Luan</a>, Yuanhao Zhai, Jingjing Meng,
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
          <a href="https://zhangchen8.github.io/">Zhang Chen</a>,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>,
          <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>.
          <br>
          <em>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b></em>, 2023
          <br>
          <p></p>

          <a href="https://cse.buffalo.edu/~jsyuan/papers/2023/6034_high_fidelity_3d_hand_shape_re-Camera-ready%20PDF.pdf">arXiv</a>
        </td>
      </tr>
      
      <tr onmouseout="RIAV_MVS_stop()" onmouseover="RIAV_MVS_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/cvpr23_riav.jpg' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="http://www.changjiangcai.com/riav-mvs.github.io/">
            <span class="papertitle">RIAV-MVS: Recurrent-Indexing an Asymmetric Volume for Multi-View Stereo</span>
          </a>
          <br>
          
        <a href="https://www.changjiangcai.com/">Changjiang Cai</a>, 
        <a href="https://sites.google.com/view/panji530">Pan Ji</a>, 
        <a href="https://yanqingan.github.io/">Qingan Yan</a>, 
        <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>.  
          <br>
          <em>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b></em>, 2023
          <br>
          <p></p>

          <a href="http://www.changjiangcai.com/riav-mvs.github.io/">project page</a>
          /
          <a href="https://www.youtube.com/watch?v=7nvtlz0Caso&ab_channel=VisionCai">video</a>
          /
          <a href="https://arxiv.org/abs/2205.14320">arXiv</a>
        </td>
      </tr>

      <tr onmouseout="georefine1_stop()" onmouseover="georefine1_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/eccv22_georefine1.jpg' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">

            <span class="papertitle">GeoRefine: Self-Supervised Online Depth Refinement for Accurate Dense Mapping</span>
          <br>
          <a href="https://sites.google.com/view/panji530">Pan Ji</a>, 
          <a href="https://yanqingan.github.io/">Qingan Yan</a>, Yuxin Ma,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu.</a>  
          <br>
          <em>European Conference on Computer Vision <b>(ECCV)</b></em>, 2022
          <br>
          <p></p>

          <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610354.pdf">arXiv</a>
          /
          <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610354-supp.pdf">Supplemental Material</a>
        </td>
      </tr>


      <tr onmouseout="PlaneMVS_stop()" onmouseover="PlaneMVS_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/cvpr22_plane.jpg' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
            <span class="papertitle">PlaneMVS: 3D Plane Reconstruction from Multi-View Stereo</span>
          <br>
        Jiachen Liu,
        <a href="https://sites.google.com/view/panji530">Pan Ji</a>, 
        <a href="https://scholar.google.com/citations?user=h_Hym_8AAAAJ&hl=en">Nitin Bansal</a>, 
        <a href="https://www.changjiangcai.com/">Changjiang Cai</a>, 
        <a href="https://yanqingan.github.io/">Qingan Yan</a>, Xiaolei Huang, 
        <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>.
          <br>
          <em>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b></em>, 2022
          <br>
          <p></p>

          <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_PlaneMVS_3D_Plane_Reconstruction_From_Multi-View_Stereo_CVPR_2022_paper.pdf">arXiv</a>
        </td>
      </tr>


      <tr onmouseout="NeuLF_stop()" onmouseover="NeuLF_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/xiaobu.gif' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://oppo-us-research.github.io/NeuLF-website/">
            <span class="papertitle">NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field. </span>
          </a>
          <br>
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
          <a href="https://lsongx.github.io/">Liangchen Song</a>,
          <a href="https://www.cct.lsu.edu/~cliu/">Celong Liu</a>,
          <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>.  
          <br>
          <em>Eurographics Symposium on Rendering<b>(EGSR)</b></em>, 2022
          <br>
          <p></p>

          <a href="https://oppo-us-research.github.io/NeuLF-website/">project page</a>
          /
          <a href="https://youtu.be/sD61yIfN6To">video</a>
          /
          <a href="https://arxiv.org/pdf/2105.07112.pdf">arXiv</a>
        </td>
      </tr>

      <tr onmouseout="TVCG2022_stop()" onmouseover="TVCG2022_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/tvcg_2022.png' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="papertitle">Real-Time Lighting Estimation for Augmented Reality via Differentiable Screen-Space Rendering.</span>
          <br>
          <a href="https://www.cct.lsu.edu/~cliu/">Celong Liu</a>, Lingyu Wang, 
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>, Shuxue Quan, 
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>.  
          <br>
          <em>IEEE Transactions on Visualization & Computer Graphics<b>(TVCG)</b></em>, 2022
          <br>
          <p></p>

          <a href="https://www.computer.org/csdl/journal/tg/5555/01/09678000/1A4SuYWCI7K">arXiv</a>
        </td>
      </tr>

      <tr onmouseout="POPNET_stop()" onmouseover="POPNET_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/popnet.png' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="papertitle">PoP-Net: Pose over Parts Network for Multi-Person 3D Pose Estimation from a Depth Image.</span>
          <br>
          <a href="https://scholar.google.com/citations?user=CP-YkUwAAAAJ&hl=en">Yuliang Guo</a>,
          <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>, Zekun Li, Xiangyu Du, Shuxue Quan,
          <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>.  
          <br>
          <em>IEEE Winter Conference on Computer Vision<b>(WACV)</b></em>, 2022
          <br>
          <p></p>

          <a href="https://arxiv.org/pdf/2012.06734.pdf">arXiv </a>/
          <a href="https://github.com/oppo-us-research/PoP-Net">dataset</a>
        </td>
      </tr>

      <tr onmouseout="MonoIndoor_stop()" onmouseover="MonoIndoor_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/iccv21_Mono.png' width="300">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
            <span class="papertitle">MonoIndoor: Towards Good Practice of Self-Supervised Monocular Depth Estimation for Indoor Environments.</span>
          <br>
        <a href="https://sites.google.com/view/panji530">Pan Ji</a>, Runze Li, Bir Bhanu,
        <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>.
          <br>
          <em>IEEE International Conference on Computer Vision<b>(ICCV)</b></em>, 2021
          <br>
          <p></p>

          <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Ji_MonoIndoor_Towards_Good_Practice_of_Self-Supervised_Monocular_Depth_Estimation_for_ICCV_2021_paper.pdf">arXiv</a>
        </td>


        <tr onmouseout="ACMMM2021_stop()" onmouseover="ACMMM2021_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/acmmm_2021.png' width="300">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <span class="papertitle">Learning Kinematic Formulas from Multiple View Videos.</span>
            <br>
            <a href="https://lsongx.github.io/">Liangchen Song</a>, Sheng Liu,
            <a href="https://www.cct.lsu.edu/~cliu/">Celong Liu</a>,
            <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>, Yuqi Ding,
            <a href="https://scholar.google.com.au/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>, 
            <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>.  
            <br>
            <em>ACM International Conf. on Multimedia<b>(ACM MM)</b></em>, 2021
            <br>
            <p></p>
  
            <a href="https://cse.buffalo.edu/~jsyuan/papers/2021/ACM_MM2021_liangchen-2.pdf">arXiv </a>
          </td>
        </tr>
      
    </table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:right;font-size:small;">
            Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron</a>
            <br>
            Last updated: Sep, 2023
          </p>
        </td>
      </tr>
    </table>
  </td>
</tr>
</table>
  </body>
</html>
